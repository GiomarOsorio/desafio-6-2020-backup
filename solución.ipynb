{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "D6-18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "XuBa7KF7NT_r"
      },
      "source": [
        "# MARATONA BEHIND THE CODE 2020\n",
        "\n",
        "## DESAFIO 6 - ANAHUAC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWjnwzCeNT_2"
      },
      "source": [
        "## Installing Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5fLvsXONT_3"
      },
      "source": [
        "!pip install scikit-learn --upgrade\n",
        "!pip install seaborn --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhAyvKzLNUAV"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNSu0fy-NUAZ"
      },
      "source": [
        "## Loading the .csv dataset from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdF70bYmNUAa"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkHAgA89NUAe"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/ForTraining.csv\n",
        "df_base_for_training = pd.read_csv(r'ForTraining.csv')\n",
        "df_base_for_training.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdD5PlF5NUAp"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/OrdenMaterias.csv\n",
        "df_orden_materias = pd.read_csv(r'OrdenMaterias.csv')\n",
        "df_orden_materias.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE00Ki4wNUAv"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaConexiones.csv\n",
        "df_tabla_conexiones = pd.read_csv(r'TablaConexiones.csv')\n",
        "df_tabla_conexiones.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X0hoYD6NUA4"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaTareas.csv\n",
        "df_tabla_tareas = pd.read_csv(r'TablaTareas.csv')\n",
        "df_tabla_tareas.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zqX4lc-NUBQ"
      },
      "source": [
        "## Uniendo DataFrames en Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITFEFVgINUBw"
      },
      "source": [
        "# El resultado de esta celda sera la union de los dos anteriores dataframes\n",
        "# usando la columna ``studentId`` como llave.\n",
        "\n",
        "df = pd.merge(\n",
        "    df_base_for_training, df_tabla_tareas, how='inner',\n",
        "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
        "    left_index=False, right_index=False, sort=False,\n",
        "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
        "    validate=None\n",
        ")\n",
        "\n",
        "df = pd.merge(\n",
        "    df, df_tabla_conexiones, how='inner',\n",
        "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
        "    left_index=False, right_index=False, sort=False,\n",
        "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
        "    validate=None\n",
        ")\n",
        "\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-axRNxLNUCL"
      },
      "source": [
        "## Pre-procesando el dataset antes de entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si5DXJoKNUCV"
      },
      "source": [
        "# Visualizando los datos faltantes del dataset antes de la primera transformación (df_data_2)\n",
        "print(\"Valores nulos antes de la transformación DropNA: \\n\\n{}\\n\".format(df.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHte7I1eNUCZ"
      },
      "source": [
        "# Aplicando la función para borrar todas las filas con valor NaN en la columna ``Graduado``:\n",
        "df2 = df.dropna(axis='index', how='all', subset=['Graduado'])"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-AMa1JQNUCj"
      },
      "source": [
        "# Visualizando los datos faltantes del dataset antes de la primera transformación (SimpleImputer) (df_data_3)\n",
        "print(\"Valores nulos antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFHdEXJ_NUCn"
      },
      "source": [
        "### Procesando valores NaN con SimpleImputer de sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvGjL9P7NUCt"
      },
      "source": [
        "impute_zeros = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy='constant',\n",
        "    fill_value=0,\n",
        "    verbose=0,\n",
        "    copy=True\n",
        ")"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOiC-Vq5NUCx"
      },
      "source": [
        "print(\"Valores nulos antes de transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n",
        "\n",
        "impute_zeros.fit(X=df2)\n",
        "\n",
        "df2 = pd.DataFrame.from_records(\n",
        "    data=impute_zeros.transform(\n",
        "        X=df2\n",
        "    ),\n",
        "    columns=df2.columns\n",
        ")\n",
        "\n",
        "print(\"Valores nulos del dataset despues de la transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVEwilBZNUC0"
      },
      "source": [
        "### Manejando variables Categoricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLP7jURaNUC1"
      },
      "source": [
        "class ScalerOrdenMaterias(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        possibility_of_passing = 1\n",
        "        decreased_possibility = 0\n",
        "        data = X.copy()\n",
        "        rows, total_cicles = data.shape\n",
        "\n",
        "        for row in range(0, rows):\n",
        "          modified_row = data.loc[row].values\n",
        "\n",
        "          if 'Sin clase' in data.loc[row].values:\n",
        "            without_class = data.loc[row].value_counts()['Sin clase']\n",
        "          else:\n",
        "            without_class = 0\n",
        "\n",
        "          decreased_possibility = 1 / (total_cicles - without_class)\n",
        "\n",
        "          for cicle in range(1, total_cicles):\n",
        "              if modified_row[cicle] == 'Sin clase' or modified_row[cicle - 1] == 'Sin clase':\n",
        "                modified_row[cicle] = 1\n",
        "              else:\n",
        "                modified_row[cicle] = possibility_of_passing\n",
        "                possibility_of_passing -= decreased_possibility\n",
        "          \n",
        "          possibility_of_passing = 1\n",
        "        return data\n",
        "\n",
        "class cicleProbability(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, Y):\n",
        "        data = X.copy()\n",
        "        data_to_search = Y.copy()\n",
        "        rows = data.shape[0]\n",
        "        new_cicle = []\n",
        "        for row in range(0,rows):\n",
        "          new_cicle.append(data_to_search.loc[(data_to_search[self.columns[0]] == data.loc[row][self.columns[0]])][data.loc[row][self.columns[1]]].values[0])\n",
        "        data[self.columns[1]] = pd.Series(new_cicle)\n",
        "        return data\n",
        "\n",
        "class compressHomework(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        data = X.copy()\n",
        "        data['%_tareas_entregadas'] = (data[self.columns[0]]) / data[self.columns[1]]\n",
        "        return data"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-ML3W3NNUDB"
      },
      "source": [
        "new_scaler = ScalerOrdenMaterias()\n",
        "\n",
        "new_scaler.fit(X=df_orden_materias)\n",
        "\n",
        "df_new_orden_materias = pd.DataFrame.from_records(\n",
        "    data=new_scaler.transform(\n",
        "        X=df_orden_materias\n",
        "    ),\n",
        "    columns=df_orden_materias.columns\n",
        ")\n",
        "\n",
        "df_new_orden_materias.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Mf2seLNUDM"
      },
      "source": [
        "cicle_transform = cicleProbability(columns=['reducido','ciclo'])\n",
        "compress_tareas = compressHomework(columns=['Tareas_Puntuales','Total_Tareas'])\n",
        "\n",
        "cicle_transform.fit(X=df2)\n",
        "compress_tareas.fit(X=df2)\n",
        "\n",
        "df3 = pd.DataFrame.from_records(\n",
        "    data=compress_tareas.transform(\n",
        "        X=cicle_transform.transform(\n",
        "            X=df2,\n",
        "            Y=df_new_orden_materias\n",
        "        )\n",
        "    ),  \n",
        "    columns=np.append(df2.columns.to_numpy(), ['%_tareas_entregadas'], axis=None)\n",
        ")\n",
        "\n",
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fmY9Kq6NUDX"
      },
      "source": [
        "### Eliminando columnas no desadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM2lNtYdNUDX"
      },
      "source": [
        "df3.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKr83v8MNUDm"
      },
      "source": [
        "df4 = df3.drop(columns=['studentId', 'reducido', 'Tareas_Puntuales', 'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas'], inplace=False)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itOW1ZIgNUDq"
      },
      "source": [
        "df4.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ist4BrW3NUD0"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9CVP7xqNUD0"
      },
      "source": [
        "## Entrenando un clasificador basado  en un Árbol de Decisión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQYtANukNUD1"
      },
      "source": [
        "### Seleccionando FEATURES y definiendo la variable TARGET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1N8T41LNUD2"
      },
      "source": [
        "df4.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7cEL9OkNUED"
      },
      "source": [
        "features = df4[\n",
        "    [\n",
        "       'ciclo', 'Calificacion_Promedio', 'Dias_Conectado',\n",
        "       'Minutos_Promedio', 'Minutos_Total', '%_tareas_entregadas'\n",
        "    ]\n",
        "]\n",
        "target = df4['Graduado']"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olGz1IJZNUEP"
      },
      "source": [
        "### Dividiendo nuestro dataset en set de Entrenamiento y Pruebas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5KNPRmNUER"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=133)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76Yq5WKNUEc"
      },
      "source": [
        "### Entrenando un modelo ``DecisionTreeClassifier()``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmJHJvmNNUEd"
      },
      "source": [
        "dtc = RandomForestClassifier(bootstrap=True,\n",
        "                             ccp_alpha=0.0,\n",
        "                             class_weight= None,\n",
        "                             criterion='gini',\n",
        "                             max_depth=8,\n",
        "                             max_features='auto',\n",
        "                             max_leaf_nodes=None,\n",
        "                             max_samples=None,\n",
        "                             min_impurity_decrease=0.0,\n",
        "                             min_impurity_split=None,\n",
        "                             min_samples_leaf=2,\n",
        "                             min_samples_split=2,\n",
        "                             min_weight_fraction_leaf=0.0,\n",
        "                             n_estimators=1000,\n",
        "                             n_jobs=None,\n",
        "                             oob_score=False,\n",
        "                             random_state=0,\n",
        "                             verbose=0,\n",
        "                             warm_start=False).fit(X_train, y_train)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0D9M4fsNUEl"
      },
      "source": [
        "### Haciendo predicciones del Sample Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4x64QWVNUEm"
      },
      "source": [
        "y_pred = dtc.predict(X_test)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cw0u1MKNUEr"
      },
      "source": [
        "### Analice la calidad del modelo a través de la matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqJhi-fVNUEz"
      },
      "source": [
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "group_names = ['`Positivo` Correto', '`Negativo` Errado', 'Falso `Positivo`', '`Negativo` Correto']\n",
        "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
        "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
        "precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
        "recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
        "f1_score  = 2*precision*recall / (precision + recall)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt=\"\")\n",
        "stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label' + stats_text)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzC8NVCqNUE6"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKML3aU2NUFA"
      },
      "source": [
        "## Scoring de la data requerida para hacer la entrega de la solución"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxUsVMzUNUFB"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/for_submission/ToBePredicted.csv\n",
        "df_to_be_predicted = pd.read_csv(r'ToBePredicted.csv')\n",
        "df_to_be_predicted.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMA0bCLXNUFF"
      },
      "source": [
        "# Uniendo los dataset\n",
        "df = pd.merge(\n",
        "    df_to_be_predicted, df_tabla_tareas, how='inner',\n",
        "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
        "    left_index=False, right_index=False, sort=False,\n",
        "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
        "    validate=None\n",
        ")\n",
        "df = pd.merge(\n",
        "    df, df_tabla_conexiones, how='inner',\n",
        "    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n",
        "    left_index=False, right_index=False, sort=False,\n",
        "    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
        "    validate=None\n",
        ")\n",
        "\n",
        "#preprocesando\n",
        "##imputando\n",
        "impute_zeros.fit(X=df)\n",
        "\n",
        "df2 = pd.DataFrame.from_records(\n",
        "    data=impute_zeros.transform(\n",
        "        X=df\n",
        "    ),\n",
        "    columns=df.columns\n",
        ")\n",
        "##manejando variables categoricas\n",
        "cicle_transform.fit(X=df2)\n",
        "compress_tareas.fit(X=df2)\n",
        "\n",
        "df3 = pd.DataFrame.from_records(\n",
        "    data=compress_tareas.transform(\n",
        "        X=cicle_transform.transform(\n",
        "            X=df2,\n",
        "            Y=df_new_orden_materias\n",
        "        )\n",
        "    ),  \n",
        "    columns=np.append(df2.columns.to_numpy(), ['%_tareas_entregadas'], axis=None)\n",
        ")\n",
        "\n",
        "# Eliminando la columna 'reducido'\n",
        "df4 = df3.drop(columns=['studentId', 'reducido', 'Tareas_Puntuales', 'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas'], inplace=False)\n",
        "\n",
        "df4.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc71V7XINUFc"
      },
      "source": [
        "y_pred = dtc.predict(df4[\n",
        "    [\n",
        "       'ciclo', 'Calificacion_Promedio', 'Dias_Conectado',\n",
        "       'Minutos_Promedio', 'Minutos_Total', '%_tareas_entregadas'\n",
        "    ]\n",
        "])\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}